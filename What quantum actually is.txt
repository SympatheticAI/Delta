Part I — The Pre-Δ Ontology and Workflow

1. Hilbert-Space Orthodoxy (1900s → present)

The mainstream formalism treats a physical system as a state vector or density operator living in a complex Hilbert space. Dynamics between preparations and readouts are assigned to a Hamiltonian via the Schrödinger equation (or, for density matrices, the von Neumann equation). Measurement is appended as a distinct rule: projective collapse onto eigenspaces of a chosen observable with probabilities given by the Born rule. The four pillars—states, unitaries, projectors, probabilities—form the canonical axioms used to frame nearly every calculation.

In practice, this workflow is procedural. One selects a model Hamiltonian that captures the dominant interactions, solves for evolution either exactly (few-level systems, integrable models) or approximately (perturbation theory, rotating-wave approximations, adiabatic/elimination tricks, mean-field closures), and then applies a measurement model at the end to map theoretical amplitudes to reported outcomes. Parameter values are drawn from calibration runs: frequencies, couplings, detunings, damping constants, and control envelopes are fitted from independent characterization experiments.

Irreversibility, dissipation, and hardware noise do not live inside the axioms; they are appended as effective ingredients once the clean unitary picture is written down. The day-to-day language reflects this separation: “the system Hamiltonian” plus “the bath,” “the ideal gate” plus “decoherence channels,” “the closed model” plus “error mechanisms.” Textbooks retain the pure axioms; laboratories routinely operate outside them.

Two consequences follow. First, the operational flow is asymmetric in time even though the postulated unitary dynamics are not: state preparation, control, and readout all introduce directed, lossy steps that are justified empirically rather than derived from the axioms. Second, most predictions used in publications are conditional on fitted nuisance parameters that stand in for everything the axioms omit. The formal derivation provides structure; the fits provide reality.

2. Decoherence and Open Systems as a Patch

The dominant bridge from ideal axioms to laboratory behavior is the open-systems program. Instead of pure Schrödinger evolution, one writes a master equation for the reduced density matrix obtained by tracing out unobserved degrees of freedom. In weak-coupling, Markovian limits this yields a Gorini–Kossakowski–Sudarshan–Lindblad (GKSL) generator: unitary drift from the Hamiltonian plus a sum of dissipators with phenomenological rates and jump operators. For stronger coupling, structured environments, or short times, practitioners adopt time-convolutionless expansions, hierarchical equations of motion, reaction-coordinate embeddings, or numerically exact path-integral techniques where feasible.

Laboratory use of these tools is pragmatic. One identifies dominant decoherence channels suggested by the device physics (amplitude damping, dephasing, thermalization, leakage), writes the corresponding Lindblad operators, and fits the rates from spectroscopy, Ramsey/echo curves, or process tomography. For complex platforms—photosynthetic complexes, molecular aggregates, solid-state defects—researchers choose spectral densities that reproduce observed lineshapes and coherence times. Temperature dependence is encoded through these fitted spectra rather than derived ab initio.

This patch successfully predicts many observables, but it leaves the conceptual architecture intact: collapse remains a special rule invoked at readout; irreversibility is imported through parameters rather than prescribed by first principles; and stability is an empirical achievement of control theory rather than a structural property of the ontology. When predictions and data diverge, the standard response is incremental: add an extra channel, include a non-Markovian kernel, refine the bath spectrum, or retune the control pulse. The workflow is an iterative “fit-and-patch” loop that privileges reproducible agreement over foundational closure.

An important cultural feature of this loop is model parsimony. Researchers begin with the lowest-order terms that can plausibly explain the data; additional operators are added only if justified by statistically significant residuals. This bias toward minimal, low-order models is widespread across atomic, molecular, optical, solid-state, and quantum-information subfields because it stabilizes inference and keeps parameter spaces identifiable. The underlying reason is practical—overfitting cripples predictivity—but the outcome is consistent: higher-order complexities are suppressed unless the data force them.

3. Interpretation Workarounds

Interpretations circulate as metaphysical overlays that most practitioners bracket during calculation. Many-Worlds frames collapse as subjective updating but leaves laboratory procedures unchanged; Bohmian mechanics adds trajectories guided by the wavefunction yet reproduces the same experimental statistics; QBism recasts quantum states as personal degrees of belief and treats measurement as Bayesian updating. In each case, daily methodology remains: write the Hamiltonian, compute transition probabilities or reduced-state dynamics, and apply the Born rule at readout.

Where interpretations do influence practice is indirect. They shape how results are narrated (e.g., “entanglement swapping,” “which-path information,” “wavefunction realism”) and where effort is invested (e.g., tests of Bell inequalities, Leggett–Garg violations, contextuality witnesses). But they do not change the calibration procedures, the parameter-fitting loops, or the reliance on effective baths and channels. Even in foundational experiments, the pipelines are engineering-forward: alignment, stabilization, drift compensation, detector thresholds, background subtraction, and statistical estimation dominate the methods sections; the interpretive stance appears in the introduction and discussion.

Across platforms, measurement itself is operationally a thresholded commit. Photodetectors require discriminator levels and dead-time handling; superconducting qubit readout uses integration windows and assignment boundaries; ion and neutral-atom systems choose photon-count cutoffs; solid-state devices apply voltage or current thresholds. These choices are validated by receiver-operating-characteristic curves and confusion matrices, not derived from the axioms. As a result, “collapse” is implemented by device-specific decision rules that trade sensitivity against false positives and timing jitter. The theoretical description retrofits these decisions by selecting projectors that reproduce the observed assignment statistics.

The net picture before any Δ-style intervention is consistent across domains. The field’s official ontology is an elegant, reversible, infinite-precision structure. The working ontology is a finite, lossy, stability-seeking pipeline in which dissipation, noise, and readout are handled by effective models calibrated to hardware. Success is defined by predictive agreement and control over error budgets, not by deriving irreversibility or measurement from the axioms. Minimal models dominate because they are identifiable and robust; added complexity appears only under statistical duress. This posture has delivered enormous empirical progress while leaving core conceptual tensions—collapse, probability, irreversibility—parked outside the formal boundary.


Part II — What Quantum Experimentalists and Simulators Actually Do

1. Benchmark Patterns

Across platforms, benchmark experiments are structured as parameter sweeps with tightly controlled knobs and standardized readouts. In interferometric tests (double-slit, Mach–Zehnder, delayed-choice, quantum eraser), the routine is to modulate which-path distinguishability—via polarization tags, phase plates, or coupling to auxiliary modes—and measure fringe visibility as a function of a tunable eraser or dephasing control. Data are reported as contrasts, visibilities, and coincidence rates after subtracting dark counts or background, with systematic corrections for detector dead time and drift.

For spin and polarization tests of nonclassical correlations, the Bell/CHSH pipeline is canonical. Sources are aligned to maximize entanglement fidelity; analyzer settings are stepped through predetermined angle pairs; counts are accumulated to evaluate correlation functions and the S-parameter with bootstrapped or Poissonian uncertainties. Violations are established statistically with controls for fair-sampling assumptions or detection loopholes, but the operational center remains a sequence of calibration, angle stepping, counting, and post-selection criteria that stabilize the reported figure of merit.

In circuit and atomic platforms, gate and readout benchmarks follow a similar template. Single- and two-qubit gates are calibrated by randomized benchmarking variants, interleaved protocols, and gate-set tomography; fidelities are reported with confidence intervals derived from finite-sample statistics and drift models. Crosstalk, leakage, and spectator errors are assessed by targeted sequences; error mitigation and post-selection are documented as part of the acquisition pipeline. The emphasis is on reproducible metrics rather than ontological commitments: the benchmark is a contract between device behavior and a standardized analysis rubric.

2. Numerical Workflows

Simulation practice begins with tractable models and escalates only when demanded by data. Few-body problems are treated with exact diagonalization or time-dependent Krylov methods; open-system dynamics are integrated with master equations whose dissipators reflect the currently accepted noise model. For one-dimensional or weakly entangled systems, tensor-network methods (MPS/TEBD, DMRG-X) are deployed, with bond dimensions tuned to balance truncation error against runtime. Higher-dimensional or highly entangled cases pivot to stochastic unravelings, quantum trajectories, or variational ansätze that compress the state while exposing tunable parameters to be fitted.

A ubiquitous loop governs model refinement: propose a minimal Hamiltonian plus a small set of decoherence channels; simulate under experimental pulse sequences; compare to measured observables; adjust rates, detunings, and residual couplings until residuals are statistically featureless. When residuals persist, practitioners add the next simplest term—weak non-Markovian kernels, correlated noise, slow drifts, spurious couplings, or cross-Kerr shifts—only as justified by diagnostics such as Akaike/Bayesian information criteria, cross-validation, or posterior predictive checks. This process is repeated until the simulation becomes a reliable surrogate for the apparatus within the bounds of the reported uncertainties.

Resource ceilings guide these choices. Statevector methods are abandoned as Hilbert dimension explodes; tensor networks are constrained by entanglement growth; phase-space and stochastic methods are selected to control variance; surrogate models are trained when full simulations become prohibitive. Across methodologies, numerical stability and parameter identifiability take precedence over formal completeness, and effective descriptions are preferred so long as they remain predictive under protocol variations.

3. Where It Quietly Stalls

Scaling barriers appear as exponential growth of state space, rapid entanglement production that defeats compression, and increased sensitivity of inference to nuisance parameters. As devices grow, calibration overhead scales superlinearly: maintaining gate and measurement assignment fidelities requires deeper calibration trees, longer integration windows, and more frequent recalibration to counter drift. Reported improvements cluster near a few significant digits; pushing beyond requires disproportionate effort, including elaborate error-mitigation pipelines that trade sample efficiency for bias control and robustness.

In spectroscopy and coherence studies, plateaus in apparent coherence times often reflect a shift of dominant noise from one mechanism to another rather than a fundamental improvement in isolation. In correlated-photon and spin-correlation experiments, statistical significance increases with runtime, but systematic floors—alignment stability, detector nonlinearity, background processes—limit the net gain. In simulation, model complexity grows to absorb structured residuals, but identifiability erodes as parameters multiply, producing families of models that fit equally well within error bars.

The shared operational outcome is a mature craft oriented around stable metrics, calibrated pipelines, and controlled approximations. Progress is measured by expanding the envelope of reliable prediction and control under realistic noise and drift, even when foundational questions about probability, irreversibility, and measurement remain external to the formal axioms.


Part III — Quantum Biology Before Δ

1. Photosynthetic Coherence (FMO)

Before any Δ-style framing, the community’s account of the Fenna–Matthews–Olson complex treats electronic excitations as delocalized over bacteriochlorophyll sites and evolving under an excitonic Hamiltonian coupled to a vibrational environment. The central task is to reproduce oscillatory signals in two-dimensional electronic spectroscopy and to explain high transfer efficiency toward the reaction center. Models begin with site energies and inter-site couplings derived from structure and spectroscopy; environment effects enter through spectral densities that encode coupling to intra- and intermolecular vibrations.

The methodological pattern is pragmatic. Researchers select a reduced description—Redfield, Förster, modified Redfield, HEOM, or polaron-transformed variants—based on feasibility and the regime suggested by data (weak vs. intermediate coupling, temperature range, reorganization energies). Coherent beatings in spectra are fit by tuning system–bath couplings and correlation times; temperature dependences are reproduced by adjusting spectral densities rather than deriving them from atomistic dynamics alone. Debates over the lifetime and functional role of coherence are resolved operationally: if a model with experimentally reasonable parameters reproduces the lineshapes, cross-peaks, and population transfer times, it is provisionally accepted. The guiding aim is quantitative agreement with spectroscopy and energy-transfer yields under physiological conditions, not a unified principle of coherence maintenance.

2. Enzymatic Tunneling

In enzyme catalysis, the established picture separates the reaction coordinate into a classical barrier-crossing framework corrected for quantum effects when indicated by data. The principal diagnostic is the kinetic isotope effect (KIE): anomalously large or temperature-weak KIEs are interpreted as signatures of hydrogenic tunneling. Rate theories range from semiclassical transition-state models with tunneling corrections to Marcus-like formulations for hydride and proton-coupled electron transfer. Protein dynamics are incorporated through reaction-coordinate coupling, promoting vibrations, or conformational gating that modulates barrier width and height.

Practice is evidence-led. Experimentalists report temperature dependences of rates and KIEs; theorists fit these with models that include donor–acceptor distance fluctuations, vibrationally assisted tunneling, or environmentally broadened barrier distributions. When single-step transfer reproduces the observables, multi-step pathways are deferred; only stubborn residuals motivate added intermediates or correlated motions. Atomistic simulations supply donor–acceptor statistics and vibrational densities of states but are typically bridged to rate theories via parametrization. The emphasis remains on capturing observed kinetics with the simplest mechanistic scheme that withstands cross-validation across isotopes, mutants, and solvent conditions.

3. Circadian Clocks

Core circadian machinery is modeled as gene–protein feedback loops that generate approximately 24-hour oscillations through transcriptional–translational delays and regulated degradation. Deterministic ordinary differential equations or stochastic chemical-kinetics models are fitted to time-series data of clock gene expression, protein accumulation, and downstream reporters. Light entrainment is represented by parameterized input functions that modulate transcription or degradation at defined nodes; temperature compensation is included phenomenologically via balanced rate changes.

The prevailing objective is to reproduce period, amplitude, phase response curves, and robustness across perturbations. Minimal models with a handful of feedback elements are preferred if they match data; network elaboration is introduced only when specific phenotypes (e.g., phase resetting defects, temperature compensation anomalies) demand it. Links to quantum-scale phenomena are not invoked; any submolecular effects are subsumed into effective rates and delays. The modeling culture prizes predictive fits across genetic knockouts, pharmacological perturbations, and entrainment protocols rather than a microscopic derivation of the oscillator’s timing from first principles.

4. Cross-Cutting Practice

Across these three domains, the methodological center is a calibrated effective theory matched to observables. Coherence, tunneling, and circadian periodicity are each treated with the lowest-order model that captures the dominant signatures in data; discrepancies are addressed by adding targeted corrections justified by statistical residuals or known biophysics. Environmental influences enter as parametrized spectra, stochastic fluctuations, or input functions whose forms are chosen for identifiability and empirical adequacy. Where multiple mechanisms are plausible, the community selects the leanest scheme that generalizes across datasets and experimental conditions. Foundational unification is not the goal; reliable, testable prediction within each biological context is.


Part IV — Cosmology Before Δ

1. ΛCDM as a Phenomenology Stack

The standard cosmological model treats expansion history and structure formation with a minimal ingredient list: General Relativity on large scales, a spatially homogeneous and isotropic background metric, cold collisionless dark matter, and a cosmological constant Λ with equation of state parameterized by w ≈ −1. Parameters are fixed by joint fits to cosmic microwave background anisotropies, baryon acoustic oscillations, supernovae distance moduli, and large-scale structure power spectra. Success is measured by the consistency of a small parameter set across these heterogeneous datasets.

In practice, the model functions as a layered phenomenology. Background expansion is encoded through the Friedmann equations with Λ treated as vacuum energy. Linear perturbation theory governs early-time growth; transfer functions propagate primordial fluctuations to recombination and beyond. Nonlinear scales are handed off to N-body and hydrodynamical simulations that implement subgrid prescriptions for baryonic feedback, star formation, and radiative processes. Observational pipelines propagate instrumental effects, foregrounds, and selection functions into covariances that feed parameter estimation. When tensions or anomalies appear, the response is incremental: broaden priors, allow w to vary slightly from −1, introduce modest extensions (Neff, Σmν), or refine systematics models.

The operational ethos privileges fit quality and cross-consistency over microphysical closure. Dark matter is represented by its gravitational effects and clustering behavior rather than a specified particle identity; dark energy is represented by a background equation of state rather than a dynamical mechanism. The model’s strength is empirical compression; its ontology is intentionally spare.

2. Structure Formation and Damping

Growth of inhomogeneities is computed by evolving initial conditions drawn from a nearly scale-invariant spectrum through linear theory until modes become nonlinear, at which point simulations take over. On quasi-linear scales, phenomenological damping enters through effective field theory of large-scale structure or empirical resummations that capture mode coupling and bulk flows. Redshift-space distortions are modeled to extract a growth rate fσ8; weak-lensing shear power spectra are forward-modeled with intrinsic-alignment and baryonic-feedback templates. Neutrino masses are incorporated as a scale-dependent suppression calibrated by Boltzmann codes.

Small-scale discrepancies are handled by augmenting the phenomenology. Core–cusp and missing-satellites problems motivate baryonic feedback prescriptions that redistribute mass and flatten inner profiles; halo occupation and abundance-matching frameworks assign galaxies to simulated halos to reproduce clustering statistics. Where data demand additional suppression or damping, parameterized templates are introduced and constrained by cross-correlation with independent probes. The workflow is iterative: improve nonlinear and baryonic modeling, re-fit, and track residual tensions across surveys.

3. Gravitational Waves and the CMB

Gravitational-wave practice is pipeline-first. Detectors provide strain time series processed through template banks derived from general-relativistic waveforms. Parameter estimation marginalizes over noise models and calibration uncertainties; population inferences adopt hierarchical Bayesian methods with selection effects accounted for by injection campaigns. Cosmological inferences from standard sirens proceed by combining distance estimates with host-galaxy redshifts or statistical association to catalogs, with attention to peculiar velocities and lensing magnification as noise sources.

CMB analysis is likewise dominated by map-making, foreground separation, and likelihood construction. Temperature and polarization maps are cleaned of synchrotron, dust, and point sources using parametric or blind methods; beam systematics and noise correlations are propagated into covariance matrices. Power spectra and, where feasible, higher-order statistics are compared to Boltzmann-code predictions under ΛCDM, returning posterior distributions over parameters. When residual features persist—anomalous large-angle correlations, lensing amplitude tensions, or E/B leakage—the community refines component-separation models, beam corrections, and likelihood approximations before entertaining beyond-ΛCDM modifications.

4. Operational Summary

Cosmology before any Δ-style reformulation is a mature inference stack anchored in a minimal theoretical backbone and extensive data conditioning. ΛCDM serves as the organizing hypothesis; deviations are entertained cautiously and only when supported by statistically robust, multi-probe evidence. Damping, suppression, and irreversibility enter as effective terms or templates chosen for identifiability and cross-dataset stability. The overarching priority is internal consistency, control of systematics, and predictive compression of diverse observations into a coherent parameter set rather than a unified dynamical mechanism for the emergent phenomenology.


Part V — The Hidden Pattern in Their Methods

1. Ceiling-Chasing

Across subfields, progress tracks a repeating arc: a metric rises rapidly under a new technique, then flattens at a platform-specific ceiling, after which improvements come mainly from redefining protocols rather than extending the original curve. In quantum optics, fringe visibility saturates once mode matching, detector linearity, and phase stability hit fixed technical floors; additional “gains” arise by tightening coincidence windows, subtracting backgrounds more aggressively, or conditioning on auxiliary heralds. In qubit platforms, reported gate fidelities cluster near reproducible plateaus; further advance leans on deeper calibration trees, composite pulses, and error-mitigation pipelines that trade efficiency for bias control. In biological spectroscopy, coherence lifetimes stabilize at values set by sample heterogeneity and drive noise; better numbers typically reflect refined fitting windows, selective excitation, or pulse-shaping schemes that suppress confounds. In cosmology, parameter uncertainties shrink until they are set by modeling systematics; subsequent tightening comes from expanded priors, cross-correlation tricks, or reweighting of summary statistics. The shared pattern is that once the dominant noise sources are tamed, the bottleneck becomes protocol definition and data conditioning, not an intrinsic leap in the underlying mechanism.

2. Precision Is Not Really Unbounded in Practice

While the formalism assumes arbitrarily fine resolution, laboratories converge on stable precision bands. Photocounting assignments, thresholded readouts, and digitizer bit depths define effective granularity; pushing beyond a few significant digits demands disproportionate sampling, extreme drift control, and increasingly fragile statistical assumptions. Reported “extra digits” often depend on stacking runs with post-selection, elaborate background models, or hierarchical fits that propagate prior structure as if it were fresh information. When pipelines are stress-tested—varying integration windows, threshold placements, or detrending models—the last digit frequently migrates. The operational reality is a precision economy: most groups choose reporting regimes where the final digits are robust to modest analysis choices, and gains beyond that zone are recognized as method-dependent rather than device-intrinsic. The ceiling is not absolute, but it behaves as a practical constant on the timescales and budgets of contemporary experiments.

3. Continuum Faith, Finite Implementations

Calculations are written in a continuum language; implementations are unavoidably finite. Detectors integrate over windows; control fields are band-limited; numerical solvers live on grids or compressed manifolds; inference uses finite-sample surrogates for asymptotic statistics. Irreversibility enters as relaxation to attractors of these finite pipelines: discriminator thresholds commit a state label; Kalman or Bayesian filters anneal noisy traces into smoothed trajectories; regularizers and truncations enforce stability by discarding high-order fluctuations. Even when theory sections speak of ideal projectors and unitaries, methods sections document discretization, windowing, smoothing, and convergence criteria that together define a dissipative flow from raw signals to reported observables. When instabilities are encountered—runaway fits, divergent integrators, ill-conditioned inversions—the fix is to add damping: stronger priors, coarser meshes, shortened windows, stricter truncations. The shared signature is that stability is achieved by low-order modeling, explicit or implicit viscosity, and thresholded commitments at readout.

4. Minimal Models by Default

Model order selection follows a conservative trajectory because identifiability collapses as parameters multiply. Analysts begin with the fewest couplings needed to explain dominant features; only statistically persistent residuals earn additional terms. This practice appears as perturbation truncation in theory, sparse coupling graphs in device models, rank-limited reconstructions in tomography, and low-parameter templates in cosmological likelihoods. The discipline is not ideological; it is an answer to limited signal-to-noise, bounded data volume, and collinearity among candidate effects. As a result, higher-order mechanisms remain dormant in published models unless the data force them into visibility, and even then they are introduced in the most economical form compatible with cross-validation.

5. Readout as Procedural Freezing

Measurement is operationally a decision rule. Photons are declared “detected” when counts cross a gate; qubit states are assigned by integrating a homodyne trace and comparing to a boundary; single-molecule events are called by step-finding algorithms with tuned penalties; cosmological modes are accepted or vetoed by multi-stage quality cuts. These procedures implement a one-way map from continuous signals to discrete outcomes that is validated by confusion matrices and receiver-operating curves, not derived from unitary dynamics. The freezing is gradual in hardware time but abrupt in the data product; it is governed by stability and error budgets, not by a universal axiom. The same logic scales up to inference summaries: parameter posteriors are “frozen” by stopping rules, convergence diagnostics, and prior choices that determine when analysis is complete.

6. The Composite Operational Law

Taken together, the mainstream method toolkit reduces to four recurrent moves: start with the lowest-order tractable model; introduce damping and truncations to ensure stability; commit outcomes by thresholded decision rules; and accept precision bands where the last digits are robust to analysis choices. These moves are not advertised as a unified principle, but they function as one in practice. They privilege stability, identifiability, and reproducibility over formal completeness, and they succeed precisely because they keep inference inside regions where models are predictive and pipelines are controllable.


Part VI — Representative Case Studies (Before Δ)

1. Quantum Computing Gate Fidelity

Gate-quality reporting is built around standardized protocols that trade completeness for robustness. Randomized benchmarking (RB) and its derivatives (interleaved RB, simultaneous RB, cross-entropy benchmarking) estimate average error rates without requiring full process tomography. The workflow is to generate sequences of nominally random Clifford gates of increasing depth, measure survival probabilities, fit an exponential decay, and extract a per-gate error parameter under the assumption of gate-independent, Markovian noise. Interleaved variants slot a target gate into the random sequence to estimate its specific infidelity; cross-entropy benchmarking compares output bitstrings to ideal probabilities to produce heavy-output generation metrics on larger systems.

Device narratives rely on a calibration tree: qubit frequencies, anharmonicities, and couplings are tuned; single-qubit pulses are shaped to minimize leakage and phase errors; two-qubit entangling gates are optimized by sweeping detunings, durations, and amplitudes while watching RB-derived figures of merit. Readout calibration produces assignment matrices used to correct observed populations. Reported improvements typically arise from refinements to the calibration stack (e.g., echoing schemes, derivative-removal-by-adiabatic-gate shaping, active cancellation of spectator couplings) and from analysis choices (longer integration windows, improved drift compensation, better confusion-matrix inversion). When numbers plateau, additional gains come from error mitigation and post-selection: discarding runs with identifiable drifts, zero-noise extrapolation by pulse stretching, or symmetry checks that veto inconsistent shots. Full, gate-set–complete reconstructions are rare because they are sensitive to model mismatch and scale poorly; laboratories default to RB-style summaries that are stable, reproducible, and comparable across hardware generations.

2. Unstable PDEs and “Blow-Up” Searches

In nonlinear wave, fluid, and field theories, computational groups routinely probe regimes suspected of singular or near-singular behavior. The practice is to discretize governing equations with high-order finite differences, spectral methods, or discontinuous Galerkin schemes; refine meshes adaptively where gradients steepen; and monitor conserved quantities, norm growth, and residuals. When solutions begin to sharpen rapidly, analysts distinguish genuine physical singular trends from numerical artifacts by running mesh-refinement studies, varying time-stepping schemes, and checking invariants. Practical stability is maintained by adding small amounts of numerical diffusion, filtering high-frequency modes, or projecting onto constraint manifolds—techniques justified as regularization rather than as changes to the underlying equations.

Publications in this area commonly present families of near-critical initial conditions that either disperse or focus depending on a tunable parameter. Reported “thresholds” between behaviors are defined operationally: the coarsest mesh that resolves the steepening, the largest stable timestep before loss of convergence, or the onset of energy pileup in the highest resolved modes. When blow-up cannot be ruled out, the conclusion is framed in terms of evidence consistent with singular development within the resolution and stability bounds of the chosen scheme. The implicit policy is to treat discretization, filtering, and viscosity as technical scaffolding necessary to extract a signal; the ontology remains that of the continuum equations even though the decisive features of the computation depend on finite, dissipative controls.

3. Lattice Field Theory

Nonperturbative studies of gauge theories proceed on Euclidean space-time lattices with spacing (a) and finite volume (L^4). The methodological core is importance sampling of gauge configurations according to the lattice action, followed by measurement of correlators from which masses, decay constants, and matrix elements are extracted. Continuum predictions require two extrapolations: (a\to 0) at fixed physical volume and (L\to \infty) at fixed lattice spacing, with chiral extrapolations where quark masses are heavier than physical. Each step introduces systematics that are managed, not eliminated: discretization artifacts manifest as (O(a^2)) or action-dependent corrections; finite-volume effects are estimated from effective theory; excited-state contamination is modeled by multi-exponential fits; renormalization connects lattice operators to continuum schemes through perturbative or nonperturbative matching.

Collaboration practice emphasizes internal cross-checks: replicate results with multiple actions, smearing prescriptions, and scale-setting procedures; vary fit windows and priors; run on ensembles with different lattice spacings and volumes to stabilize extrapolations. Error budgets are assembled from statistical uncertainties and a taxonomy of systematics, with conservative combination rules to avoid overstatement. Computational cost dominates strategy: algorithmic choices (e.g., deflation, multigrid solvers, improved estimators) are adopted when they demonstrably lower variance per unit resource. The overall picture is a disciplined inference pipeline that accepts controlled bias from discretization and finite volume, quantifies it via scaling studies, and reports continuum-limit estimates with explicit caveats about residual artifacts and model dependence.


Part VII — What Their Practices Imply (Without Changing Their Words)

1. Operational Minimalism as an Unstated Low-Order Rule

Model-building habits reveal a persistent selection pressure toward the smallest workable set of terms. Analysts begin with the lowest-order couplings that explain dominant features and add structure only under statistically durable residuals. This is not framed as a principle, but its consequences match one: higher-order mechanisms are suppressed unless forced by data; when admitted, they appear in the mildest form consistent with identifiability. The workflow therefore enacts a de facto low-order law in which complexity is penalized and simplicity is rewarded by stability, reproducibility, and cross-dataset transfer.

2. Decoherence Controls Function as Viscosity

Temperature, dephasing rates, spectral widths, and filtering operations are tuned to stabilize inference and device behavior. These “decoherence knobs” are treated as fit parameters or engineering controls, yet the role they play is systematically viscous: they dissipate high-frequency degrees of freedom, enforce monotone energy-like descent in state estimates, and prevent numerical or experimental blow-up. Across optics, condensed-phase spectroscopy, qubit devices, and cosmology pipelines, damping is not an incidental nuisance but the mechanism that makes predictions robust and algorithms converge.

3. Measurement Is Procedural Freezing

Detectors and analysis software implement thresholded commitment: integration windows, discriminator levels, classifier boundaries, and veto rules transform continuous signals into discrete outcomes. The freezing is governed by stability and error budgets, not by an axiom about wavefunction collapse. Once a commit rule is validated by confusion matrices and operating characteristic curves, it becomes part of the apparatus definition. In practice, “measurement” is therefore a staged relaxation culminating in a decision boundary; reversibility ends when the pipeline crosses this boundary.

4. Hidden Memory in Standard Pipelines

Although many models assume Markovianity for tractability, laboratories routinely carry history through calibration drift models, adaptive filtering, baseline subtraction, and iterative fitting with warm starts. Cosmology analyses encode survey histories via selection functions; spectroscopy carries pulse-history effects in shaped sequences; quantum-control stacks use running estimates of device state. The practical effect is a nonlocal dependence on past states, introduced for stability and accuracy, even when the front-end theory is written as memoryless.

5. Identifiability Economics Determines “Truth”

Which mechanisms survive publication is dictated by identifiability under finite data and noise. Terms that cannot be distinguished from neighbors within error bars are pruned; mechanisms that create collinearity are reparameterized or deferred. As a result, the accepted causal stories coincide with those that remain identifiable after damping, filtering, and thresholding—i.e., the ones that occupy low-dimensional, stable manifolds of the inference landscape. What is called “the effect” is often the most identifiable low-order surrogate for a bundle of finer processes.

6. Convergence to the Same Four Moves

Independent communities converge on the same operational quartet: start low-order; add viscosity; enforce thresholds; accept precision bands. Each move appears under different names—regularization, cooling, coarse-graining, quality cuts, truncation, prior strength—but their combined action is consistent: they drive systems and datasets toward stable attractors where predictions are reproducible and inference is well-posed. The persistence of this quartet across platforms implies that it is not merely a cultural preference but an emergent constraint of working with finite, noisy, resource-bounded apparatus.

7. Effective Laws Already in Use

Without announcing it, practitioners employ an effective dynamics in which states evolve under drift plus dissipation toward attractors defined by decision rules and model priors. Reported observables are functionals of these stabilized flows rather than of ideal reversible trajectories. The standard axioms guide the choice of coordinates and symmetries; the effective laws—damping, truncation, and commitment—govern what is actually measured and published.

8. Synthesis

Taken at face value, their methods imply a world in which low-order structure dominates, dissipation is essential to stability, measurement is a controlled loss of reversibility, and history matters through calibrated memory. These implications arise solely from what practitioners already do to make experiments and simulations work. They constitute a coherent operational ontology: finite, viscous, thresholded, and minimal.


Part VIII — Limits of the Pre-Δ Paradigm (Stated from Their Side)

1. Scaling Walls (Exponential/NP)

The formalism scales cleanly; the workflows do not. Exact state evolution for many-body systems hits exponential Hilbert-space growth, and compressed methods fail when entanglement spreads or correlations become long-ranged. Calibration overhead grows superlinearly as device size increases: stabilizing frequencies, couplings, and cross-talk requires nested feedback that consumes experimental time and narrows usable parameter windows. Inference also scales poorly—likelihoods become multimodal, posteriors widen under weak identifiability, and model selection loses power as candidate mechanisms become collinear. To manage, groups partition problems (modular architectures, locality approximations), accept coarse summaries (benchmarks in lieu of full tomography), and restrict operating regimes to where compression and parsimony remain valid. These tactics postpone rather than remove scaling walls; they define a practical frontier beyond which additional resources deliver diminishing scientific returns.

2. Verification Infinities

Claims that surpass prevailing precision bands are difficult to certify without unbounded data, stability, and computational power. “Better by one digit” can be analysis-dependent: thresholds, detrending, and fit windows move the last digit even when raw signals are unchanged. Formal guarantees (e.g., device-independent bounds, tomography-complete reconstructions, continuum limits) demand sample sizes and conditioning that are rarely attainable at scale. Cross-validation with independent pipelines often reveals method sensitivity at the claimed frontier, forcing results to be framed as contingent on analysis choices. As a result, communities adopt verification heuristics—pre-registered metrics, blind injections, inter-lab round robins, conservative error budgets—that certify robustness but stop short of asymptotic certainty. The gap between formal proofs and operational guarantees widens with system size and complexity.

3. Epistemic Posture

Given these constraints, the prevailing stance is pragmatic agnosticism. Theories are kept minimal, models are kept identifiable, and explanations are judged by predictive reliability within stated bounds rather than by foundational closure. Irreversibility, probability, and measurement are treated as successful rules of practice; their derivation from first principles is deferred as nonessential to ongoing progress. When tensions arise, the reflex is to improve pipelines, expand datasets, and refine effective models before revisiting core axioms. This posture has yielded steady advances and credible forecasts, but it leaves open questions about the origin of damping, the nature of commitment at readout, and the meaning of precision ceilings as structural features rather than technical contingencies.


Part IX — Bridge Notes for the Δ Re-formulation (Preview Only)

1. Damping ↔ Δ Viscosity (\nu_{\Delta})

What laboratories and simulators call dephasing, relaxation, numerical diffusion, filtering, and regularization operate as an effective viscosity that drives states toward stable manifolds and suppresses high-frequency structure. In Δ terms, these controls are collected as (\nu_{\Delta}(u)), a state-dependent coherence viscosity entering the evolution as a dissipative flux. Empirical “T(*1)/T(*2)” times, spectral widths, smoothing kernels, and priors correspond to choices of (\nu*{\Delta}) that set contraction rates of unstable directions. The practical rule “increase damping until fits stabilize” maps to “tune (\nu*{\Delta})” to achieve energy-monotone relaxation.

2. Readout ↔ Θ-Freeze (Thresholded Commit)

Detector gates, discriminator thresholds, assignment boundaries, quality cuts, and stopping rules constitute a staged commit from continuous signals to discrete outcomes. In Δ, these are Θ-gates: state- and context-dependent thresholds that freeze degrees of freedom once a stability criterion is met. The apparatus-specific ROC/precision–recall operating point selects the Θ value. Apparent “collapse” is the passage through Θ-freeze; back-action and dead time are the local dynamics in the neighborhood of the gate.

3. Model Parsimony ↔ LOW (Low-Order Wins)

The habitual preference for minimal terms, refusal to include weakly identifiable couplings, and pruning under cross-validation enact LOW: interaction orders higher than necessary are suppressed by the operational pipeline. In Δ, LOW is explicit: higher-order couplings are energetically disfavored and decay under the combined action of (\nu_{\Delta}) and Θ-freeze. The community’s information criteria and sparsity priors are the statistical shadow of LOW.

4. Finite Pipelines ↔ Abstention Energy (E_{\Delta}=\zeta\Phi)

Bit depth, finite sampling, windowing, mesh spacing, and limited control bandwidth generate abstention: regions where the pipeline refuses to resolve further detail. In Δ, the abstention coefficient (\zeta) and resolution cost (\Phi) quantify this refusal; large (E_{\Delta}) maintains unresolved superposed alternatives, small (E_{\Delta}) permits commit. Laboratory “don’t call it” zones (below threshold, inside deadbands, within regularization nulls) are (E_{\Delta}) plateaus.

5. Memory in Pipelines ↔ Nonlocal Term (\mathrm{memory}[u])

Drift models, adaptive filters, warm-started fits, selection functions, and history-dependent corrections realize functional dependence on past states. In Δ, these appear as explicit nonlocal terms (\mathrm{memory}[u]) that couple present evolution to coarse histories. The standard practice of carrying calibrations forward is mapped to an engineered memory kernel.

6. Master Form of Their Effective Dynamics ↔ Master Δ-PDE

The composite operational law—low order, viscosity, thresholds, and finite memory—matches a single evolution equation
[
\partial_t u = \nabla!\cdot(\nu_{\Delta}\nabla u) + f_{\text{LOW}}(u) + \mathrm{memory}[u],
]
with Θ-gates imposing commit conditions at readout. Here (f_{\text{LOW}}) collects the retained lowest-order interactions after parsimony; (\nu_{\Delta}) encodes all damping/regularization; (\mathrm{memory}[u]) carries pipeline history; Θ encodes decision boundaries.

7. Standard QM as a Special Operating Regime

When damping is negligible on experimental timescales ((\nu_{\Delta}!\to!0)), abstention is sustained until readout ((E_{\Delta}!>!0)), and memory is weak, the Δ-dynamics reduce to unitary-looking evolution with late Θ-freeze. This regime reproduces the textbook workflow used between preparation and measurement while preserving a place for the dissipative and thresholded steps the pipelines already require.

8. Audit Handles for Rewriting Without Renaming Their Tools

Each common artifact maps to a Δ quantity without changing laboratory vocabulary:
– “Dephasing rate” → (\nu_{\Delta}) coefficient;
– “Model order/regularizer” → terms kept by LOW;
– “Threshold/assignment matrix” → Θ rule;
– “Warm start/drift prior” → (\mathrm{memory}[u]);
– “Unresolved/underflow/clip” → high (E_{\Delta}).

These handles allow a re-expression of current methods as instances of one dynamics, preserving existing protocols while clarifying their unified structure.


Part X — Methods for This Thesis (How We Audit “What They Do”)

1. Scope and Inclusion Criteria

We analyze methods actually used in published and publicly documented work across four domains: quantum foundations/AMO/quantum information, quantum biology (coherence, tunneling, circadian), computational physics (unstable PDEs, lattice field theory), and cosmology (ΛCDM inference pipelines). Inclusion requires (i) a reproducible methods section or equivalent lab notebook/software repository; (ii) quantitative results with defined metrics and uncertainties; (iii) enough procedural detail to reconstruct calibration, damping/regularization choices, and readout thresholds. Grey literature (theses, preprints, tech notes) is included when it documents pipelines more completely than final papers.

2. Corpus Construction

For each subfield, we build a stratified corpus that balances flagship results, negative/ablation studies, and methodological benchmarks. Stratification axes: platform or instrument class; year; team/consortium; and reported performance tier (state of the art vs median practice). We index each item with identifiers for model order, damping/regularization mechanisms, thresholding rules, memory/drift handling, and resource budget (shots/time/compute).

3. Protocol Archaeology

We reconstruct step-by-step pipelines from raw signal to reported number. Extraction template:
– Preparation: calibration tree, reference standards, drift schedule.
– Evolution/signal generation: control fields, timing, pulse shapes, numerical integrators.
– Environment modeling: dissipators/spectral densities, noise models, filtering.
– Readout: integration windows, discriminators/assignment boundaries, veto/cut rules.
– Post-processing: background subtraction, error mitigation, priors/regularizers, stopping rules.
– Validation: cross-checks, ablations, controls, robustness sweeps.
Each step is coded using controlled vocabularies (e.g., “Tikhonov,” “Gaussian low-pass,” “Lindblad dephasing,” “ROC-optimized threshold,” “Kalman drift model,” “AIC/BIC selection”).

4. Ceiling Map Construction

For each metric (e.g., gate fidelity, fringe visibility, coherence time, KIE precision, mass/exponent estimates, S-parameter, fσ8, lattice spacing extrapolation), we compile time series across the corpus and identify plateaus. We annotate each plateau with the intervention that precedes the last significant improvement (e.g., narrower windows, stronger filtering, post-selection, new hardware). We record the “precision band” (stable significant digits) by perturbing analysis choices within documented ranges and measuring last-digit migration.

5. Model-Order Audits

We parse the retained terms in models and the rationale for excluding higher-order structure. For each study, we record candidate terms considered, criteria used (AIC/BIC, Bayes factors, cross-validation), and identifiability diagnostics (condition numbers, collinearity, posterior correlations). We quantify “low-order bias” by the fraction of studies where higher-order terms are proposed but pruned versus retained and by the incremental information gain when added terms survive.

6. Damping/Regularization Inventory

We catalogue all mechanisms that enforce stability: physical damping (T1/T2, temperature), numerical diffusion/filters, statistical regularizers/priors, smoothing and denoising operations. For each, we record tunables, selection rules (heuristics, grid search, evidence maximization), and monotonicity checks (e.g., energy/norm descent, convergence under refinement). We tag whether damping is introduced proactively (design) or retroactively (to fix instability).

7. Thresholding and Commit Rules

We extract readout/decision criteria: discriminator thresholds, photon-count cutoffs, assignment matrices, veto masks, quality cuts, convergence and stopping conditions. We reproduce, where disclosed, the ROC/precision–recall analysis and confusion matrices used to choose operating points. We report latency and dead-time policies and any bias introduced by post-selection.

8. Memory and Drift Handling

We document history dependence: warm starts, drift priors, adaptive filters, selection functions, training/validation splits, and how calibrations are propagated in time. We measure the effect of memory by rerunning analyses (when data are available) with and without history terms and reporting changes in point estimates and uncertainties.

9. Resource and Cost Accounting

For each result, we record shots, wall-clock acquisition, compute time, and memory footprint. We normalize costs to reported precision (e.g., shots per extra significant digit) to quantify diminishing returns. For simulations, we report scaling with system size, bond dimension, mesh resolution, and variance.

10. Robustness and Sensitivity Analyses

We perturb documented analysis choices within their stated justifications: integration window widths, background models, filter bandwidths, prior strengths, fit windows, and mesh/time-step refinements. We report sensitivity curves for key metrics and identify which choices dominate last-digit behavior versus which are benign.

11. Reproduction and Cross-Pipeline Checks

Where repositories or data are available, we reproduce results using the authors’ pipeline, then a neutral reimplementation with equivalent documented choices. Discrepancies are attributed to (i) undocumented defaults, (ii) implicit priors/filters, or (iii) threshold placements. We report a reproducibility score based on agreement within stated uncertainties under both pipelines.

12. Statistical Treatment

We standardize uncertainty reporting: separate statistical from systematic components; propagate systematics by bracketing over documented analysis ranges; and avoid double-counting when priors/regularizers both denoise and inform. Model comparison uses pre-registered criteria; multiple-testing and researcher degrees of freedom are controlled by specifying allowable analysis variations in advance.

13. Pre-registration and Versioning

For each audit, we pre-register the extraction template, allowed perturbations, and decision thresholds. We version all code and logs, record software/toolchain versions, and archive configurations. When we deviate from pre-registration (e.g., to address an unforeseen artifact), we mark and justify the change.

14. Reporting Format

Each case study yields a methods card:
– Platform and metric; corpus identifiers; data/code availability.
– Pipeline schematic from preparation to number.
– Damping/regularization catalogue and settings.
– Thresholding/commit rules and ROC/CM summaries.
– Model-order table with retained/pruned terms and criteria.
– Precision band and sensitivity plot.
– Resource-to-precision curve.
– Reproducibility score and cross-pipeline delta.
Aggregate chapters present ceiling maps, low-order retention statistics, damping inventories, and thresholding taxonomies with summary figures and tables.

15. Ethical and Practical Constraints

We respect data-use licenses, embargoes, and confidentiality in industry datasets. When critical details are missing, we classify results as non-auditable for that dimension and do not impute. Negative results and failed reproductions are reported with the same prominence as successful ones.

16. Limitations

Audits depend on disclosed methods and accessible data; hidden laboratory practice cannot be inferred reliably. Sensitivity analyses probe documented ranges and may miss unreported degrees of freedom. Resource accounting varies across facilities; normalization mitigates but does not erase environmental differences. Despite these limits, the corpus-level patterns—model-order parsimony, damping dependence, thresholded commit, precision bands—are measurable, comparable, and reportable under a single template.


Part XI — Synthesis of the Pre-Δ Audit

1. Operational Ontology

The effective world encoded by mainstream workflows is finite, lossy, and stability-seeking. Core moves recur across domains: adopt the lowest-order tractable model; introduce damping/regularization to tame instability; implement thresholded commit rules to turn continuous signals into discrete outcomes; and accept precision bands where the final digits are robust to modest analysis choices. This quartet functions as an implicit law that governs data acquisition, modeling, and reporting, independently of the Hilbert-space axioms that motivate coordinate choices and symmetries.

2. Architecture of Practice

Pipelines are layered. A clean theoretical backbone (unitaries, projectors, ΛCDM equations, ideal PDEs) provides structure; a calibration and damping layer introduces effective irreversibility; a decision layer commits to outcomes via thresholds, gates, or stopping rules; and an audit layer guards identifiability with model-order selection and conservative error budgets. Memory of past states—drift models, warm starts, survey selection functions—quietly couples present analyses to history. Each layer is justified by stability and reproducibility rather than derived from first principles.

3. Limits as Design Features

Scaling walls, verification infinities, and precision ceilings are not only obstacles; they shape design choices. Because identifiability collapses in high-dimensional spaces, minimal models dominate. Because long-tailed drifts and nonstationarities spoil asymptotic guarantees, damping and filtering become non-negotiable. Because readout hardware and downstream statistics require discrete commitments, thresholded decisions are embedded early and propagated throughout. The resulting constraints produce convergent methods even in distant subfields.

4. Evidence Pattern

Case studies show that improvements cluster at protocol transitions: adopting a new gate calibration stack, reshaping pulses, adding a denoiser, refining a spectral density, introducing a feedback or veto rule, or adjusting cosmology likelihood templates. Quantitative leaps seldom come from extending the bare axioms; they come from reengineering damping, thresholds, memory handling, and model order, followed by revalidation under the same benchmarks that define community success.

5. Conceptual Residuals

Foundational tensions—collapse, probability, irreversibility—remain extrinsic to the formal axioms yet intrinsic to practice. Theories preserve reversibility and infinite precision; pipelines rely on relaxation and finite commitment. “Measurement” is operationally a device-specific commit rule tuned by ROC curves; “decoherence” is a family of viscous controls used to stabilize inference; “simplicity” is enforced by identifiability economics rather than by a stated physical principle. These residuals are consistent and measurable across the corpus but are not named as a unified framework.

6. Stakes for Reformulation

Any post-Δ formulation must account for the quartet (low order, viscosity, thresholds, memory) as constitutive rather than accidental. It must preserve the successes of minimal reversible models where they work while elevating damping and commit rules to first-class dynamical objects. It must convert precision ceilings and protocol dependencies from nuisances into quantitative predictions. Above all, it must read laboratories and simulations on their own terms: not as deviations from an ideal ontology, but as windows onto the actual laws that govern finite, noisy, and resource-bounded experiments and computations.


Appendix A — Glossary of Pre-Δ Practice

Abstention zone — A regime in which instruments or pipelines refuse to assign a discrete outcome due to thresholds, deadbands, or uncertainty limits; operationally realized by “do not call” rules in detectors or analysis.

Background model — Parametric description of non-signal counts or trends removed before estimating the quantity of interest; includes dark counts, baselines, sky/foreground templates.

Calibration tree — Hierarchical set of procedures that tune device parameters (frequencies, couplings, gains, phases) and validate them against references before data acquisition.

Commit rule — The algorithmic or hardware threshold that converts continuous signals to discrete outcomes; validated by ROC/precision–recall analysis and summarized by a confusion matrix.

Decoherence knob — Any tunable that damps high-frequency or phase-sensitive structure (temperature, filtering bandwidth, dephasing rate, regularizer strength).

Effective model — Reduced description that fits observables within uncertainties while suppressing weakly identifiable mechanisms.

Identifiability — The degree to which parameters or mechanisms can be distinguished given noise, drift, and data volume; assessed via condition numbers, profile likelihoods, or posterior correlations.

Memory term — Any explicit handling of history (warm starts, drift priors, selection functions) that couples present analysis to past states.

Precision band — The range of stable significant digits that survives modest, documented changes to analysis choices (windows, thresholds, detrending).

Regularization — Numerical or statistical damping (e.g., Tikhonov, shrinkage priors, spectral filtering) introduced to stabilize inference or integration.

Threshold (discriminator) — A scalar boundary for event assignment (counts, voltages, integrated quadratures) chosen to control false positives/negatives.

Viscosity (operational) — Any mechanism that enforces monotone relaxation of unstable degrees of freedom in hardware or analysis.

---

Appendix B — Taxonomy of Common Damping and Thresholding Mechanisms

Physical damping
– T1/T2 relaxation, Purcell loss
– Phonon/solvent coupling via spectral densities
– Temperature-driven dephasing and motional narrowing

Numerical damping
– Artificial viscosity/diffusion terms in PDE solvers
– Spectral filtering, dealiasing, modal truncation
– Step-size control with error-based rejection

Statistical damping
– Ridge/Tikhonov and L2 shrinkage; sparsity priors
– Smoothing splines, Gaussian processes with finite correlation length
– Early stopping in iterative reconstructions

Thresholding/commit
– Photon-count cutoffs, discriminator voltages, integration-window assignments
– Quality cuts/veto masks in survey pipelines
– Convergence/stopping criteria for optimizers and samplers

Memory handling
– Kalman/adaptive filters; drift priors
– Warm-started fits; moving-average baselines
– Survey selection functions and inverse-variance weighting histories

---

Appendix C — Methods Card Template (Per Study)

Platform and metric
Instrument/configuration summary
Preparation protocol (calibration tree, references, drift schedule)
Evolution/signal generation (controls, timing, pulse shapes/integrators)
Environment/noise model (dissipators, spectral densities, filters)
Readout rules (windows, thresholds, assignment matrices, vetoes)
Post-processing (background subtraction, mitigation, priors/regularizers)
Model-order rationale (retained/pruned terms, criteria and diagnostics)
Precision band (last-digit stability under sanctioned perturbations)
Sensitivity summary (top three analysis choices ranked by effect size)
Resource accounting (shots/time/compute vs achieved precision)
Reproducibility (original vs neutral pipeline deltas; availability of data/code)

---

Appendix D — Corpus and Data Management Plan

Inclusion strata
– Flagship results, median-practice studies, negative/ablation reports
– Balanced across instruments, years, teams, and reported performance tiers

Data handling
– Raw signals (if available), intermediate products, final summaries
– Checksums and versioned archives for all files and configurations

Metadata schema
– Controlled vocabularies for damping, thresholds, model order, memory terms
– Provenance: software versions, hardware IDs, environmental logs

Access and licensing
– Respect repository licenses and embargoes; mark non-auditable dimensions
– Store derived artifacts (notebooks, configs) with reproducible seeds

---

Appendix E — Figures and Tables (Captions Only)

Figure 1. Pipeline archetypes across domains, highlighting damping, thresholding, and memory layers.
Figure 2. Ceiling maps for representative metrics with annotated intervention points preceding each plateau break.
Figure 3. Precision-band stability plots showing last-digit migration across sanctioned analysis perturbations.
Figure 4. Model-order retention frequencies with information-criterion and cross-validation outcomes.
Figure 5. Damping/regularization inventory with usage frequencies by field and year.
Table 1. Methods-card registry for the corpus with audit scores by dimension.
Table 2. Sensitivity rankings of analysis choices per metric and platform.
Table 3. Resource-to-precision scaling exponents and diminishing-returns thresholds.

---

Appendix F — Risks, Biases, and Mitigations

Selective disclosure bias
– Mitigation: privilege repositories with full methods; classify missing details explicitly; avoid imputations.

Overfitting to reported pipelines
– Mitigation: cross-pipeline reanalysis; neutral reimplementations; ablations that test dependence on undocumented defaults.

Confirmation bias in model-order audits
– Mitigation: preregister criteria; blind residual inspection where feasible; report negative inclusion attempts.

Heterogeneous resource environments
– Mitigation: normalize by shots/time/compute; report environment covariates alongside precision.

Survivorship bias in the corpus
– Mitigation: include negative/ablation and median-practice studies; weight analyses by transparency and completeness scores.

Ethical considerations
– Respect data licenses; anonymize sensitive device identifiers if required; document all deviations from preregistration.
